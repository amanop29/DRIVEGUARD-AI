#!/usr/bin/env python3
"""
GPU/CPU Status Checker for DriveGuard AI
Run this script to check if your system is using GPU or CPU for video analysis
"""

import torch
import sys
import os

def check_gpu_status():
    """Check and display GPU/CPU status"""
    
    print("=" * 70)
    print("ğŸ” DriveGuard AI - GPU/CPU Status Check")
    print("=" * 70)
    print()
    
    # PyTorch Info
    print("ğŸ“¦ System Information:")
    print(f"   PyTorch Version: {torch.__version__}")
    print(f"   Python Version: {sys.version.split()[0]}")
    print()
    
    # Check available devices
    print("ğŸ–¥ï¸  Available Devices:")
    print("-" * 70)
    
    # CUDA Check (NVIDIA GPU)
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print("   ğŸ® CUDA (NVIDIA GPU): âœ… Available")
        print(f"      Device: {torch.cuda.get_device_name(0)}")
        print(f"      Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("   ğŸ® CUDA (NVIDIA GPU): âŒ Not available")
    
    print()
    
    # MPS Check (Apple Silicon)
    mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    if mps_available:
        print("   ğŸ MPS (Apple Silicon): âœ… Available")
        print("      Your Mac has GPU acceleration!")
    else:
        print("   ğŸ MPS (Apple Silicon): âŒ Not available")
    
    print()
    
    # CPU (always available)
    print("   ğŸ’» CPU: âœ… Always available (fallback)")
    print()
    
    # Determine active device
    print("=" * 70)
    print("ğŸ¯ Active Device Configuration:")
    print("=" * 70)
    
    if cuda_available:
        active_device = "CUDA (NVIDIA GPU)"
        icon = "ğŸ®"
        speedup = "5-10x faster than CPU"
    elif mps_available:
        active_device = "MPS (Apple Silicon GPU)"
        icon = "ğŸ"
        speedup = "2-4x faster than CPU"
    else:
        active_device = "CPU"
        icon = "ğŸ’»"
        speedup = "Baseline performance"
    
    print()
    print(f"   {icon} Device: {active_device}")
    print(f"   âš¡ Performance: {speedup}")
    print()
    
    # Check if scripts are configured for GPU
    print("=" * 70)
    print("ğŸ“ Script Configuration Check:")
    print("=" * 70)
    print()
    
    script_path = os.path.join(os.path.dirname(__file__), 'main_v2.py')
    if os.path.exists(script_path):
        try:
            import main_v2
            if hasattr(main_v2, 'DEVICE'):
                script_device = main_v2.DEVICE
                print(f"   âœ… main_v2.py configured")
                print(f"   ğŸ“ Using: {script_device.upper()}")
                
                if script_device == 'mps' and mps_available:
                    print("   ğŸ‰ GPU ACCELERATION ACTIVE IN SCRIPTS!")
                elif script_device == 'cuda' and cuda_available:
                    print("   ğŸ‰ GPU ACCELERATION ACTIVE IN SCRIPTS!")
                elif script_device == 'cpu':
                    print("   â„¹ï¸  Scripts configured for CPU")
                    if mps_available or cuda_available:
                        print("   âš ï¸  GPU available but not being used!")
            else:
                print("   âš ï¸  DEVICE variable not found in main_v2.py")
        except Exception as e:
            print(f"   âš ï¸  Could not check main_v2.py: {e}")
    else:
        print(f"   âŒ main_v2.py not found at: {script_path}")
    
    print()
    
    # Performance test
    print("=" * 70)
    print("ğŸ§ª Quick Performance Test:")
    print("=" * 70)
    print()
    
    import time
    
    # CPU test
    print("   Testing CPU performance...")
    start = time.time()
    x_cpu = torch.randn(2000, 2000)
    y_cpu = torch.matmul(x_cpu, x_cpu)
    cpu_time = time.time() - start
    print(f"   ğŸ’» CPU Time: {cpu_time:.4f} seconds")
    
    # GPU test
    if mps_available:
        print("   Testing GPU (MPS) performance...")
        try:
            start = time.time()
            x_gpu = torch.randn(2000, 2000, device='mps')
            y_gpu = torch.matmul(x_gpu, x_gpu)
            torch.mps.synchronize()
            gpu_time = time.time() - start
            print(f"   ğŸ GPU Time: {gpu_time:.4f} seconds")
            speedup_actual = cpu_time / gpu_time
            print(f"   âš¡ Speedup: {speedup_actual:.2f}x faster")
        except Exception as e:
            print(f"   âš ï¸  GPU test failed: {e}")
    elif cuda_available:
        print("   Testing GPU (CUDA) performance...")
        try:
            start = time.time()
            x_gpu = torch.randn(2000, 2000, device='cuda')
            y_gpu = torch.matmul(x_gpu, x_gpu)
            torch.cuda.synchronize()
            gpu_time = time.time() - start
            print(f"   ğŸ® GPU Time: {gpu_time:.4f} seconds")
            speedup_actual = cpu_time / gpu_time
            print(f"   âš¡ Speedup: {speedup_actual:.2f}x faster")
        except Exception as e:
            print(f"   âš ï¸  GPU test failed: {e}")
    
    print()
    
    # Summary
    print("=" * 70)
    print("ğŸ“Š Summary:")
    print("=" * 70)
    print()
    
    if (mps_available or cuda_available):
        print("   âœ… GPU acceleration is available and active!")
        print("   âš¡ Your videos will process 2-4x faster")
        print("   ğŸ¯ DriveGuard AI is optimized for performance")
    else:
        print("   ğŸ’» Running on CPU")
        print("   â„¹ï¸  Processing will be slower")
        print("   ğŸ’¡ Consider using a Mac with Apple Silicon for GPU acceleration")
    
    print()
    print("=" * 70)
    print("âœ… Status check complete!")
    print("=" * 70)
    print()


if __name__ == "__main__":
    check_gpu_status()
